{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "744c9987-142e-444a-a759-289443330e7f",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b3bf842-3497-49db-b6a2-f9ee03d008af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-14 17:21:43.115348: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e32f30c3-9942-496c-a139-da66b8e36c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you have train error, try this:\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df668028-2b2d-442b-ad3f-bfc105186b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical devices cannot be modified after being initialized\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            \n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a07546-3edd-4c0b-8fa9-e195604ab164",
   "metadata": {},
   "source": [
    "# Model initilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af16854d-9ad1-436f-b643-2dc3a5dae02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'gpt2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "847d9806-d405-469f-9ff9-e23243bcd56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = transformers.GPT2Config.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5310a01a-f20e-4658-b8cb-29634bfde0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = transformers.TFGPT2LMHeadModel.from_pretrained(MODEL_NAME)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ecd6ce1-d720-4350-8f09-34dfbf3b2938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc = [[11246  6827]]\n"
     ]
    }
   ],
   "source": [
    "enc = tokenizer(['some sentence'], add_special_tokens=True, return_tensors='np', max_length=512, truncation=True)\n",
    "print('enc =', enc['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90897023-2322-440a-8bfb-17a301824211",
   "metadata": {},
   "source": [
    "Cheking EOS token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38cb07a5-0241-4334-9d7c-303ae480a99c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|endoftext|>']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[key for key, value in tokenizer.get_vocab().items() if value == 50256]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf91a7f-3c9e-4d09-8e81-291a4046f828",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5fb4e7-e33b-44a3-b694-e368267d4de6",
   "metadata": {},
   "source": [
    "For this purpose i am using Spotify Million Song Dataset from kaggle (https://www.kaggle.com/datasets/notshrirang/spotify-million-song-dataset). It contains song names, artists names, link to the song and lyrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10beef75-b566-459a-9f64-0fdafe604e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Spotify Million Song Dataset_exported.csv').drop('link', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9a4d03-6b05-48f5-af16-447cbd05bf52",
   "metadata": {},
   "source": [
    "Creating a dataframe and filling it with data in form [sentence, attention_mask]. In this case\n",
    "I am using only texts, but it is possible to add name of song in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "974d67c0-1f6f-4311-aaa9-6b6f5f5bfa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd82725d-576a-4c35-b622-56c11beb2478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(x):\n",
    "    # tokenization\n",
    "    tokenized = tokenizer(x, add_special_tokens=False, return_tensors='np')\n",
    "\n",
    "    # adding eos to ids and mask\n",
    "    tokenized['input_ids'] = np.append(tokenized['input_ids'], 50256)\n",
    "    tokenized['attention_mask'] = np.append(tokenized['attention_mask'], 1)\n",
    "    \n",
    "    return tokenized['input_ids'], tokenized['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43e80260-2f4a-47f3-aba7-3239ae024683",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df:\n",
    "    if col == 'text':\n",
    "        tokenized_df[['sentence', 'attention_mask']] = df[col].apply(encode_text).apply(pd.Series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81426da0-b2b0-490d-a82a-82a28aa0f302",
   "metadata": {},
   "source": [
    "Using tensorflow dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b78bf75e-9742-4c05-ac22-79a0f4177631",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffb4552-e2cb-42cd-ba50-fb08c43183d4",
   "metadata": {},
   "source": [
    "Defining dataset form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "069bb65a-191e-4d89-ae58-8f07f856181a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataset(texts, attention_mask):\n",
    "    source = {\"input_ids\": texts,\n",
    "              \"attention_mask\": attention_mask}\n",
    "    target = texts\n",
    "    \n",
    "    return (source, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f3518b9-64ea-461e-9d27-577f585f460b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_df = tokenized_df.sample(frac=1).reset_index(drop=True) #shuffle ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438a8d59-0ca7-4c16-a4e6-e780554e827b",
   "metadata": {},
   "source": [
    "Padding sequences to length 512 (with this model possible is up to 1024 tokens). Using values -1 for texts and 0 for attention mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c1db6e2-5fcc-45e8-a03c-73caa94face2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(df, batch_size=1, maxlen = 512): #batch_size = 1 due to lack of video memory\n",
    "\n",
    "    texts = tf.constant(pad_sequences(df['sentence'], maxlen = maxlen, truncating = \"post\", padding = \"post\", value = -1))\n",
    "    mask = tf.constant(pad_sequences(df['attention_mask'], maxlen = maxlen, truncating = \"post\", padding = \"post\", value = 0))\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((texts, mask))\n",
    "    \n",
    "    return dataset.shuffle(2048) \\\n",
    "                  .batch(batch_size) \\\n",
    "                  .map(format_dataset) \\\n",
    "                  .prefetch(16).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03bc767c-4c46-472c-a547-2e3f3469fafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train validation split\n",
    "train_ds = make_dataset(tokenized_df.head(55650))\n",
    "val_ds = make_dataset(tokenized_df.tail(2000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7537cf86-8c5f-4887-a25b-8ea538e4d3bf",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "195e67ed-868f-43ab-ba65-4cc9e63193b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97b8f954-efa7-47d4-a469-6a8a46a84cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(3e-5)) # compiling model. Don't use loss, as it says in the documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92ebbc8-9d1c-4182-aab0-0a67e4c684ba",
   "metadata": {},
   "source": [
    "I`ve fited model for 1 epoch. It is possible that more epochs will lead to better generation, but it is time consuming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "842901a1-1117-4236-a3d6-c7c667aea60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-15 10:38:13.748905: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f33d4025600 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-02-15 10:38:13.748967: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1080, Compute Capability 6.1\n",
      "2024-02-15 10:38:13.753728: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-02-15 10:38:13.895741: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2024-02-15 10:38:14.008685: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55650/55650 [==============================] - 10865s 195ms/step - loss: 1.3896 - val_loss: 1.3301\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f34002ca3a0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_ds, validation_data = val_ds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5add245e-e21a-4841-9176-7074262c1dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fine_tuned_gpt2/tokenizer_config.json',\n",
       " 'fine_tuned_gpt2/special_tokens_map.json',\n",
       " 'fine_tuned_gpt2/vocab.json',\n",
       " 'fine_tuned_gpt2/merges.txt',\n",
       " 'fine_tuned_gpt2/added_tokens.json',\n",
       " 'fine_tuned_gpt2/tokenizer.json')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saving model\n",
    "output_dir = 'fine_tuned_gpt2'\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469a6586-ae5f-430b-bcec-20f4123f3b00",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bff8570b-78b2-4324-b9b3-ca00732581a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-14 17:21:51.950525: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-14 17:21:51.952260: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-14 17:21:51.952685: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-14 17:21:51.954311: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-14 17:21:51.954701: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-14 17:21:51.955001: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-14 17:21:53.591703: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-14 17:21:53.592300: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-14 17:21:53.592314: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1726] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-03-14 17:21:53.592537: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-14 17:21:53.592575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6256 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:07:00.0, compute capability: 6.1\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at fine_tuned_gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "pipe = transformers.pipeline(task='text-generation', model='fine_tuned_gpt2', )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb23028d-671a-4e19-9b84-06d18cd616c5",
   "metadata": {},
   "source": [
    "To generate lyrics write down start_point (can be empty)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71397913-3e2d-4374-8231-cec663319ff8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_point = \"\"\n",
    "\n",
    "#max_length - length of the returned sentence. A longer length requires more time to generate.\n",
    "#num_return_sequences - the number of sequences generated.\n",
    "#temperature - this parameter defines how diverse or \"crazy\" the model will be in its predictions.\n",
    "#do_sample - this parameter defines whether the model will use top-p/top-k sampling. It allows the use of more rare tokens during generation.\n",
    "generated = pipe(start_point, max_length=128, num_return_sequences=1, return_full_text=True, temperature = 0.9, do_sample = True)\n",
    "\n",
    "#print(start_point)\n",
    "for sent in generated:\n",
    "     print(sent['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d75552-bd0b-40cf-a058-4a750d2806bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
